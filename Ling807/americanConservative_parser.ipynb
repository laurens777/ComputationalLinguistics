{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up\n",
    "\n",
    "Import `requests` to retrieve the website data\n",
    "Import `BeautifulSoup` to sift through the html data and extract the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the Article Links\n",
    "\n",
    "The following code loops through the pages of the political news section of the American Conservative. For each page it retrieves the links to the articles and stores each link in the `article_links` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links = []\n",
    "    \n",
    "i = 1\n",
    "for x in range(100):\n",
    "    my_session = requests.session()\n",
    "    for_cookies = my_session.get(\"https://www.theamericanconservative.com/web-categories/politics/page/%s/\" % i)\n",
    "    my_cookies = for_cookies.cookies\n",
    "    my_headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.87 Safari/537.36'}\n",
    "    response = my_session.get(\"https://www.theamericanconservative.com/web-categories/politics/page/%s/\" % i, headers=my_headers, cookies=my_cookies)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    article_links += soup.find_all(class_=\"c-card-default\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Retrieval Code\n",
    "\n",
    "First we open a text file where we can store all of the text we want from the articles.  \n",
    "\n",
    "Secondly we use nested for loops to get the actual content. The first for loop retrieves a single url for an article from the list of articles we compiled in the previous section. We request the raw data from the article. Then we use a second for loop to write the text from each paragraph in the main body to the text file.\n",
    "\n",
    "Lastly we must close the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"test.txt\", \"w\")\n",
    "\n",
    "for item in article_links:\n",
    "    if item.find('a').get('href').split('/')[1] == \"web-categories\":\n",
    "        continue\n",
    "    my_session = requests.session()\n",
    "    for_cookies = my_session.get(item.find('a').get('href'))\n",
    "    my_cookies = for_cookies.cookies\n",
    "    my_headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.87 Safari/537.36'}\n",
    "    response = my_session.get(item.find('a').get('href'), headers=my_headers, cookies=my_cookies)\n",
    "    content = BeautifulSoup(response.content, 'html.parser')\n",
    "    for pp in content.select(\"div.c-content > p\"):\n",
    "        file.write(pp.text + \" \")\n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
