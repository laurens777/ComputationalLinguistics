{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up\n",
    "\n",
    "First we import the pertinent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests\n",
    "import nltk\n",
    "import textblob\n",
    "import csv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first line we open the textfile that we would like to analyze.\n",
    "\n",
    "We load the spacyNLP Named Entity Recognizer as well as the Vader Sentiment Model from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"washington_times.txt\", \"r\").read()\n",
    "sent = sent_tokenize(text)\n",
    "\n",
    "spacyNLP = spacy.load('en_core_web_sm')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Data\n",
    "\n",
    "For each sentence in our data we check if it contains any named entities that are actual persons. \n",
    "If there is only one \"person\" named entity in the sentence then we extract that entity and find their political affiliation on Wikipedia. \n",
    "\n",
    "Lastly we run sentiment analysis on this sentence and write all this information to a spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in sent:\n",
    "    j = spacyNLP(i)\n",
    "    persons = 0\n",
    "    for element in j.ents:\n",
    "        if(element.label_ == 'PERSON'):\n",
    "            persons += 1\n",
    "    if(persons > 1):\n",
    "        continue\n",
    "    \n",
    "    for element in j.ents:\n",
    "        if(element.label_ == 'PERSON'):\n",
    "            x = element.text.split()\n",
    "            if(len(x) == 2):\n",
    "                page = requests.get(\"https://en.wikipedia.org/wiki/\" + x[0] + \"_\" + x[1])\n",
    "                soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                table = soup.find('table')\n",
    "                if table is None:\n",
    "                    continue\n",
    "                party = table.find('th', string=\"Political party\")\n",
    "                if(party is not None):\n",
    "                    ss = sid.polarity_scores(i)\n",
    "                    results = x[0] + \" \" + x[1] + \",\" + party.find_next_sibling().text.split()[0] + \",\" + str(ss[\"compound\"]) + \"\\n\"\n",
    "                    with open('results_american_conservative.csv', \"a\") as out:\n",
    "                        out.write(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
