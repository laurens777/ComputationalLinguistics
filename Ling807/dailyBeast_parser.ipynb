{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up\n",
    "\n",
    "Import `requests` to retrieve the website data.    \n",
    "Import `time` for the `sleep` function that allows us to pause code.\n",
    "\n",
    "Import `selenium` and `webdriver` to load the relevant webpages.    \n",
    "Import `BeautifulSoup` to sift through the html data and extract the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the Article Links\n",
    "\n",
    "First we set various variables that the webdriver needs to load the webpage properly.\n",
    "\n",
    "The while loop clicks the \"load more\" button `i` times.\n",
    "After all content is made available we use beautiful soup to extract the links to the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b284eb7b52a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChromeOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'headless'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutable_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr\"/home/laurens/Downloads/chromedriver_linux64/chromedriver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.thedailybeast.com/category/politics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "\n",
    "driver = webdriver.Chrome(options=options, executable_path=r\"/home/laurens/Downloads/chromedriver_linux64/chromedriver\")\n",
    "driver.get(\"https://www.thedailybeast.com/category/politics\")\n",
    "\n",
    "i=0\n",
    "while driver.find_element_by_css_selector(\"Button.LoadMoreButton\") and i<100:\n",
    "    driver.find_element_by_css_selector(\"Button.LoadMoreButton\").click()\n",
    "    time.sleep(1)\n",
    "    i += 1\n",
    "\n",
    "links = driver.find_elements_by_css_selector('a.GridStory__image-link')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Retrieval Code\n",
    "\n",
    "First we open a text file where we can store all of the text we want from the articles.  \n",
    "\n",
    "Secondly we use nested for loops to get the actual content. The first for loop retrieves a single url for an article from the list of articles we compiled in the previous section. We request the raw data from the article. Then we use a second for loop to write the text from each paragraph in the main body to the text file.\n",
    "\n",
    "Lastly we must close the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"DB.txt\", \"w\")\n",
    "for item in links:\n",
    "    article = requests.get(item.get_attribute(\"href\"))\n",
    "    content = BeautifulSoup(article.text, 'html.parser')\n",
    "    for pp in content.select(\"div.Mobiledoc > p\"):\n",
    "        file.write(pp.text + \" \")\n",
    "    time.sleep(5)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
